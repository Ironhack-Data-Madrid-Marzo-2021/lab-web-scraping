{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "from urllib.request import urlopen\n",
    "# import random\n",
    "import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael Lynch (mtlynch)',\n",
       " 'Mike Penz (mikepenz)',\n",
       " 'van Hauser (vanhauser-thc)',\n",
       " 'Tobias Koppers (sokra)',\n",
       " 'Bastian Köcher (bkchr)',\n",
       " 'Tom Kerkhove (tomkerkhove)',\n",
       " 'Jonatan Kłosko (jonatanklosko)',\n",
       " 'Tanner Linsley (tannerlinsley)',\n",
       " 'Will (willscott)',\n",
       " 'Robert Mosolgo (rmosolgo)',\n",
       " 'Evan Wallace (evanw)',\n",
       " 'An Tao (an-tao)',\n",
       " 'Hajime Hoshi (hajimehoshi)',\n",
       " 'Robin Malfait (RobinMalfait)',\n",
       " 'Thibault Duplessis (ornicar)',\n",
       " 'Brandon Bayer (flybayer)',\n",
       " 'Jef LeCompte (jef)',\n",
       " 'GO Sueyoshi (sue445)',\n",
       " 'Brad Fitzpatrick (bradfitz)',\n",
       " 'Dustin L. Howett (DHowett)',\n",
       " 'Frost Ming (frostming)',\n",
       " 'Mike McQuaid (MikeMcQuaid)',\n",
       " 'David Peter (sharkdp)',\n",
       " 'Christian Bromann (christian-bromann)',\n",
       " 'Andrew Gallant (BurntSushi)']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'\n",
    "html= requests.get(url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "#your code\n",
    "nombres=[title.find('a').text.strip() for title in soup.findAll(\"h1\", {\"class\":\"h3 lh-condensed\"})]\n",
    "nicks=[title.find('a').text.strip() for title in soup.findAll(\"p\", {\"class\":\"f4 text-normal mb-1\"})]\n",
    "nom_nick=zip(nombres,nicks)\n",
    "lista=[f\"{n} ({nk})\" for n,nk in nom_nick]\n",
    "lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chia-Network/Chia-Network',\n",
       " 'd2l-ai/d2l-ai',\n",
       " 'microsoft/microsoft',\n",
       " 'huggingface/huggingface',\n",
       " 'apache/apache',\n",
       " 'bes-dev/bes-dev',\n",
       " 'ansible/ansible',\n",
       " 'deepset-ai/deepset-ai',\n",
       " 'confluentinc/confluentinc',\n",
       " 'awslabs/awslabs',\n",
       " 'Shawn-Shan/Shawn-Shan',\n",
       " 'AlexxIT/AlexxIT',\n",
       " 'soimort/soimort',\n",
       " 'Forescout/Forescout',\n",
       " 'deepmind/deepmind',\n",
       " 'mtlynch/mtlynch',\n",
       " 'PaddlePaddle/PaddlePaddle',\n",
       " 'PyTorchLightning/PyTorchLightning',\n",
       " 'coqui-ai/coqui-ai',\n",
       " 'PaddlePaddle/PaddlePaddle',\n",
       " 'devicons/devicons',\n",
       " 'blakeblackshear/blakeblackshear',\n",
       " 'lyhue1991/lyhue1991',\n",
       " 'jazzband/jazzband',\n",
       " 'ultralytics/ultralytics']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'\n",
    "\n",
    "#your code\n",
    "html= requests.get(url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "repos=[title.find('a').text.strip() for title in soup.findAll(\"h1\", {\"class\":\"h3 lh-condensed\"})]\n",
    "#repos_tit=[r for r in soup.find(\"span\", {\"class\":\"text-normal\"})]\n",
    "titulos=[r.split('/') for r in repos]\n",
    "juntos=[t[0].strip()+'/'+t[0].strip() for t in titulos]\n",
    "juntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " '/static/images/footer/wikimedia-button.png',\n",
       " '/static/images/footer/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "html= requests.get(url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "imagenes = soup.find_all('img') #he buscado expresamente todas las imágenes de la página, si quiero osolo las del recueadro aquí haríamos un paso\n",
    "url_imagenes=[i['src'] for i in imagenes]\n",
    "print(len(url_imagenes)) \n",
    "url_imagenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Pythons',\n",
       " 'https://en.wikipedia.org/wiki/Python_(genus)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(programming_language)',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Aenus',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Efteling)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(automobile_maker)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(missile)',\n",
       " 'https://en.wikipedia.org/wiki/PYTHON',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Monty)_Pictures',\n",
       " 'https://en.wikipedia.org/wiki/Cython']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' \n",
    "html= requests.get(url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "caja_enlaces = soup.find('div', {\"id\":\"mw-content-text\"})\n",
    "sub_caja_enlaces=caja_enlaces.find('div',{\"class\":\"mw-parser-output\"})\n",
    "\n",
    "enlaces=[e.find('a')['href'] for e in sub_caja_enlaces.find_all('ul')]\n",
    "\n",
    "prefix_url='https://en.wikipedia.org'\n",
    "enlaces_prefix=[prefix_url+e for e in enlaces]\n",
    "computing=enlaces_prefix.pop(2)\n",
    "enlaces_prefix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El total de cambios es 4\n"
     ]
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "html= requests.get(url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "changes=len(soup.find_all('div',{\"class\":\"usctitlechanged\"}))\n",
    "print(f\"El total de cambios es {changes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>reg</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>14.9 N</td>\n",
       "      <td>94.0 W</td>\n",
       "      <td>OAST OF CHIAPAS, MEXICO</td>\n",
       "      <td>18:10:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>35.6 N</td>\n",
       "      <td>117. W</td>\n",
       "      <td>ERN CALIFORNIA</td>\n",
       "      <td>18:05:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>41.5 N</td>\n",
       "      <td>19.5 E</td>\n",
       "      <td>TIC SEA</td>\n",
       "      <td>17:48:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>39.3 N</td>\n",
       "      <td>26.1 E</td>\n",
       "      <td>THE COAST OF WESTERN TURKEY</td>\n",
       "      <td>17:45:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>39.3 N</td>\n",
       "      <td>26.1 E</td>\n",
       "      <td>THE COAST OF WESTERN TURKEY</td>\n",
       "      <td>17:27:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>39.2 N</td>\n",
       "      <td>26.1 E</td>\n",
       "      <td>THE COAST OF WESTERN TURKEY</td>\n",
       "      <td>17:17:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>46.2 N</td>\n",
       "      <td>7.39 E</td>\n",
       "      <td>ERLAND</td>\n",
       "      <td>17:07:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>18.9 N</td>\n",
       "      <td>68.7 W</td>\n",
       "      <td>ICAN REPUBLIC</td>\n",
       "      <td>16:53:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>51.3 N</td>\n",
       "      <td>100. E</td>\n",
       "      <td>A-MONGOLIA BORDER REGION</td>\n",
       "      <td>16:42:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>2.47 S</td>\n",
       "      <td>79.4 W</td>\n",
       "      <td>COAST OF ECUADOR</td>\n",
       "      <td>16:41:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>11.0 N</td>\n",
       "      <td>121. E</td>\n",
       "      <td>, PHILIPPINES</td>\n",
       "      <td>16:36:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>34.7 N</td>\n",
       "      <td>24.9 E</td>\n",
       "      <td>, GREECE</td>\n",
       "      <td>16:36:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>9.37 N</td>\n",
       "      <td>126. E</td>\n",
       "      <td>NAO, PHILIPPINES</td>\n",
       "      <td>16:28:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>36.4 N</td>\n",
       "      <td>27.0 E</td>\n",
       "      <td>ANESE IS.-TURKEY BORDER REG</td>\n",
       "      <td>16:15:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>35.2 S</td>\n",
       "      <td>74.1 W</td>\n",
       "      <td>OAST OF MAULE, CHILE</td>\n",
       "      <td>16:10:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>19.1 N</td>\n",
       "      <td>155. W</td>\n",
       "      <td>D OF HAWAII, HAWAII</td>\n",
       "      <td>16:05:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>0.08 S</td>\n",
       "      <td>128. E</td>\n",
       "      <td>HERA, INDONESIA</td>\n",
       "      <td>16:00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>0.42 N</td>\n",
       "      <td>123. E</td>\n",
       "      <td>ASA, SULAWESI, INDONESIA</td>\n",
       "      <td>15:35:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>021-04-15</td>\n",
       "      <td>40.0 N</td>\n",
       "      <td>45.2 E</td>\n",
       "      <td>IA</td>\n",
       "      <td>15:26:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date     lat     lon                          reg        time\n",
       "0   021-04-15   14.9 N  94.0 W      OAST OF CHIAPAS, MEXICO    18:10:04\n",
       "1   021-04-15   35.6 N  117. W               ERN CALIFORNIA    18:05:03\n",
       "2   021-04-15   41.5 N  19.5 E                      TIC SEA    17:48:41\n",
       "3   021-04-15   39.3 N  26.1 E  THE COAST OF WESTERN TURKEY    17:45:05\n",
       "4   021-04-15   39.3 N  26.1 E  THE COAST OF WESTERN TURKEY    17:27:52\n",
       "5   021-04-15   39.2 N  26.1 E  THE COAST OF WESTERN TURKEY    17:17:04\n",
       "6   021-04-15   46.2 N  7.39 E                       ERLAND    17:07:31\n",
       "7   021-04-15   18.9 N  68.7 W                ICAN REPUBLIC    16:53:26\n",
       "8   021-04-15   51.3 N  100. E     A-MONGOLIA BORDER REGION    16:42:02\n",
       "9   021-04-15   2.47 S  79.4 W             COAST OF ECUADOR    16:41:02\n",
       "10  021-04-15   11.0 N  121. E                , PHILIPPINES    16:36:53\n",
       "11  021-04-15   34.7 N  24.9 E                     , GREECE    16:36:19\n",
       "12  021-04-15   9.37 N  126. E             NAO, PHILIPPINES    16:28:05\n",
       "13  021-04-15   36.4 N  27.0 E  ANESE IS.-TURKEY BORDER REG    16:15:27\n",
       "14  021-04-15   35.2 S  74.1 W         OAST OF MAULE, CHILE    16:10:46\n",
       "15  021-04-15   19.1 N  155. W          D OF HAWAII, HAWAII    16:05:02\n",
       "16  021-04-15   0.08 S  128. E              HERA, INDONESIA    16:00:15\n",
       "17  021-04-15   0.42 N  123. E     ASA, SULAWESI, INDONESIA    15:35:48\n",
       "18  021-04-15   40.0 N  45.2 E                           IA    15:26:18"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "import pandas as pd\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "html= requests.get(url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "caja=soup.find('tbody',{\"id\":\"tbody\"})\n",
    "lista_terremotos=caja.find_all(\"tr\")[0:19]\n",
    "\n",
    "list_lat=list()\n",
    "list_lon=list()\n",
    "list_date=list()\n",
    "list_time=list()\n",
    "list_reg=list()\n",
    "\n",
    "for t in lista_terremotos:\n",
    "    \n",
    "    latlon = t.find_all('td',{\"class\":\"tabev1\"})\n",
    "    lat = (latlon[0]).text[0:4]\n",
    "    lon = (latlon[1]).text[0:4]\n",
    "    \n",
    "    NSEW = t.find_all('td',{\"class\":\"tabev2\"})\n",
    "    list_lat.append((lat+' '+NSEW[0].text[0]))\n",
    "    list_lon.append((lon+' '+NSEW[1].text[0]))\n",
    "    \n",
    "    list_date.append((t.find('td',{\"class\":\"tabev6\"}).find(\"a\").text)[1: 11])\n",
    "    list_time.append((t.find('td',{\"class\":\"tabev6\"}).find(\"a\").text)[-12: -2])\n",
    "    list_reg.append(t.find('td',{\"class\":\"tb_region\"}).text[6::])\n",
    "    \n",
    "d={'date':list_date , 'lat':list_lat , 'lon':list_lon , 'reg':list_reg , 'time':list_time}\n",
    "pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'\n",
    "url_hack = 'https://hackevents.co/search/anything/anywhere/anytime' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "#no puedo acceder a la web :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English   1\n",
      "Español   2\n",
      "Deutsch   3\n",
      "日本語   4\n",
      "Русский   5\n",
      "Français   6\n",
      "Italiano   7\n",
      "中文   8\n",
      "Português   9\n"
     ]
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "\n",
    "#your code\n",
    "html= requests.get(url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "for i in range(1,10):\n",
    "    clase='central-featured-lang lang'+str(i)\n",
    "    idioma=soup.find(\"div\",{\"class\":clase}).find(\"strong\").text\n",
    "    print(idioma,' ',i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n",
    "#your code\n",
    "html= requests.get(url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "body = soup.find(\"ul\",{\"class\":\"govuk-list dgu-topics__list\"})\n",
    "group_db = [lang.text for lang in body.find_all(\"a\")]\n",
    "group_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idioma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mandarin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bengali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Idioma\n",
       "1     Mandarin\n",
       "2      Spanish\n",
       "3      English\n",
       "4        Hindi\n",
       "5          [a]\n",
       "6       Arabic\n",
       "7   Portuguese\n",
       "8      Bengali\n",
       "9      Russian\n",
       "10    Japanese"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "import pandas as pd\n",
    "#your code\n",
    "html= requests.get(url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "tabla=soup.find_all(\"table\", {\"class\":\"wikitable sortable\"})\n",
    "cuerpo_tabla = tabla[1].find(\"tbody\")\n",
    "idiomas=[fila.text for fila in cuerpo_tabla.find_all(\"a\")]\n",
    "\n",
    "df_diomas = pd.DataFrame(idiomas[0:10], columns=[\"Idioma\"], index = range(1,11))\n",
    "df_diomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>in_release</th>\n",
       "      <th>director</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>[ Tim Robbins,  Morgan Freeman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>[ Marlon Brando,  Al Pacino]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>[ Al Pacino,  Robert De Niro]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>[ Christian Bale,  Heath Ledger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>[ Henry Fonda,  Lee J. Cobb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Sucedió una noche</td>\n",
       "      <td>1934</td>\n",
       "      <td>Frank Capra</td>\n",
       "      <td>[ Clark Gable,  Claudette Colbert]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Milagro en la celda 7</td>\n",
       "      <td>2019</td>\n",
       "      <td>Mehmet Ada Öztekin</td>\n",
       "      <td>[ Aras Bulut Iynemli,  Nisa Sofiya Aksongur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Mandarinas</td>\n",
       "      <td>2013</td>\n",
       "      <td>Zaza Urushadze</td>\n",
       "      <td>[ Lembit Ulfsak,  Elmo Nüganen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>La batalla de Argel</td>\n",
       "      <td>1966</td>\n",
       "      <td>Gillo Pontecorvo</td>\n",
       "      <td>[ Brahim Hadjadj,  Jean Martin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Swades: We, the People</td>\n",
       "      <td>2004</td>\n",
       "      <td>Ashutosh Gowariker</td>\n",
       "      <td>[ Shah Rukh Khan,  Gayatri Joshi]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name in_release               director  \\\n",
       "0           Cadena perpetua       1994        Frank Darabont    \n",
       "1                El padrino       1972  Francis Ford Coppola    \n",
       "2      El padrino: Parte II       1974  Francis Ford Coppola    \n",
       "3       El caballero oscuro       2008     Christopher Nolan    \n",
       "4     12 hombres sin piedad       1957          Sidney Lumet    \n",
       "..                      ...        ...                    ...   \n",
       "245       Sucedió una noche       1934           Frank Capra    \n",
       "246   Milagro en la celda 7       2019    Mehmet Ada Öztekin    \n",
       "247              Mandarinas       2013        Zaza Urushadze    \n",
       "248     La batalla de Argel       1966      Gillo Pontecorvo    \n",
       "249  Swades: We, the People       2004    Ashutosh Gowariker    \n",
       "\n",
       "                                            stars  \n",
       "0                 [ Tim Robbins,  Morgan Freeman]  \n",
       "1                    [ Marlon Brando,  Al Pacino]  \n",
       "2                   [ Al Pacino,  Robert De Niro]  \n",
       "3                [ Christian Bale,  Heath Ledger]  \n",
       "4                    [ Henry Fonda,  Lee J. Cobb]  \n",
       "..                                            ...  \n",
       "245            [ Clark Gable,  Claudette Colbert]  \n",
       "246  [ Aras Bulut Iynemli,  Nisa Sofiya Aksongur]  \n",
       "247               [ Lembit Ulfsak,  Elmo Nüganen]  \n",
       "248               [ Brahim Hadjadj,  Jean Martin]  \n",
       "249             [ Shah Rukh Khan,  Gayatri Joshi]  \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "import pandas as pd\n",
    "def take_movie_info(l):\n",
    "    info={'name':list(),\n",
    "             'in_release':list(),\n",
    "             'director':list(),\n",
    "             'stars':list()}\n",
    "\n",
    "    for m in l:\n",
    "        info['name'].append(m.find(\"td\",{\"class\":\"titleColumn\"}).find(\"a\").text)\n",
    "        info['in_release'].append(m.find(\"td\",{\"class\":\"titleColumn\"}).find(\"span\").text[1:5])\n",
    "        \n",
    "        stars=m.find(\"td\",{\"class\":\"titleColumn\"}).find(\"a\")['title'].split(',')\n",
    "        info['director'].append(stars[0][:-6])\n",
    "        info['stars'].append(stars[1:])\n",
    "        \n",
    "    return info\n",
    "\n",
    "html= requests.get(url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "tabla = soup.find(\"table\", {\"class\":\"chart full-width\"})\n",
    "lista_pelis=[peli for peli in tabla.find_all(\"tr\")]\n",
    "lista_pelis=lista_pelis[1:]\n",
    "df=pd.DataFrame(take_movie_info(lista_pelis))\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
